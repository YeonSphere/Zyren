module bootstrap.lexer

type TokenKind {
    // Keywords
    FN
    TYPE
    IMPORT
    RETURN
    LET
    
    // Literals
    IDENTIFIER
    NUMBER
    STRING
    
    // Symbols
    LPAREN    // (
    RPAREN    // )
    LBRACE    // {
    RBRACE    // }
    ARROW     // ->
    EQUALS    // =
    COMMA     // ,
    DOT       // .
    
    EOF       // End of file
}

type Token {
    kind: TokenKind
    value: string
    line: int
    column: int
}

type Lexer {
    source: string
    current: int
    start: int
    line: int
    column: int
}

fn tokenize(source: string) -> Result<[]Token> {
    let lexer = Lexer{
        source: source,
        current: 0,
        start: 0,
        line: 1,
        column: 1
    }
    
    let tokens = []
    while !is_at_end(lexer) {
        lexer.start = lexer.current
        scan_token(lexer, tokens)
    }
    
    tokens.push(Token{
        kind: TokenKind.EOF,
        value: "",
        line: lexer.line,
        column: lexer.column
    })
    
    return Ok(tokens)
}

fn scan_token(lexer: &mut Lexer, tokens: &mut []Token) {
    let c = advance(lexer)
    match c {
        '(' => add_token(lexer, tokens, TokenKind.LPAREN),
        ')' => add_token(lexer, tokens, TokenKind.RPAREN),
        '{' => add_token(lexer, tokens, TokenKind.LBRACE),
        '}' => add_token(lexer, tokens, TokenKind.RBRACE),
        '=' => add_token(lexer, tokens, TokenKind.EQUALS),
        ',' => add_token(lexer, tokens, TokenKind.COMMA),
        '.' => add_token(lexer, tokens, TokenKind.DOT),
        '-' => {
            if match_next(lexer, '>') {
                add_token(lexer, tokens, TokenKind.ARROW)
            }
        },
        '"' => string(lexer, tokens),
        ' ' | '\r' | '\t' => {}, // Skip whitespace
        '\n' => new_line(lexer),
        _ => {
            if is_digit(c) {
                number(lexer, tokens)
            } else if is_alpha(c) {
                identifier(lexer, tokens)
            } else {
                error(lexer, "Unexpected character")
            }
        }
    }
}

fn identifier(lexer: &mut Lexer, tokens: &mut []Token) {
    while is_alphanumeric(peek(lexer)) {
        advance(lexer)
    }
    
    let text = lexer.source[lexer.start..lexer.current]
    let kind = match text {
        "fn" => TokenKind.FN,
        "type" => TokenKind.TYPE,
        "import" => TokenKind.IMPORT,
        "return" => TokenKind.RETURN,
        "let" => TokenKind.LET,
        _ => TokenKind.IDENTIFIER
    }
    
    add_token(lexer, tokens, kind)
}