module compiler.lexer

import compiler.lexer.token.{Token, TokenType}
import core.types.{Result, Option}
import core.string.{is_whitespace, is_digit, is_alpha}

type Lexer {
    source: string
    file: string
    current: int
    start: int
    line: int
    column: int
    tokens: []Token
}

fn new_lexer(source: string, file: string) -> Lexer {
    Lexer {
        source: source,
        file: file,
        current: 0,
        start: 0,
        line: 1,
        column: 1,
        tokens: []
    }
}

fn run(self: &mut Lexer) -> Result<[]Token> {
    while !self.is_at_end() {
        self.start = self.current
        self.scan_token()
    }
    
    // Add EOF token
    self.add_token(TokenType.EOF, "")
    return Ok(self.tokens)
}

fn scan_token(self: &mut Lexer) {
    char = self.advance()
    
    match char {
        '(' => self.add_token(TokenType.LPAREN, "("),
        ')' => self.add_token(TokenType.RPAREN, ")"),
        '{' => self.add_token(TokenType.LBRACE, "{"),
        '}' => self.add_token(TokenType.RBRACE, "}"),
        '[' => self.add_token(TokenType.LBRACKET, "["),
        ']' => self.add_token(TokenType.RBRACKET, "]"),
        ',' => self.add_token(TokenType.COMMA, ","),
        '.' => self.add_token(TokenType.DOT, "."),
        ':' => self.add_token(TokenType.COLON, ":"),
        
        // Operators
        '+' => self.add_token(TokenType.PLUS, "+"),
        '-' => self.match_next('>', TokenType.ARROW, TokenType.MINUS),
        '*' => self.add_token(TokenType.STAR, "*"),
        '/' => self.add_token(TokenType.SLASH, "/"),
        '=' => self.add_token(TokenType.EQUALS, "="),
        
        // String literals
        '"' => self.string(),
        
        // Whitespace
        ' ' | '\r' | '\t' => {}, // Skip whitespace
        '\n' => self.new_line(),
        
        // Numbers or identifiers
        _ => {
            if is_digit(char) {
                self.number()
            } else if is_alpha(char) {
                self.identifier()
            } else {
                return Err("Unexpected character")
            }
        }
    }
}

fn identifier(self: &mut Lexer) {
    while is_alpha(self.peek()) || is_digit(self.peek()) {
        self.advance()
    }
    
    text = self.source[self.start..self.current]
    type = self.keyword_type(text).unwrap_or(TokenType.IDENTIFIER)
    self.add_token(type, text)
}

fn keyword_type(text: string) -> Option<TokenType> {
    match text {
        "fn" => Some(TokenType.FN),
        "type" => Some(TokenType.TYPE),
        "import" => Some(TokenType.IMPORT),
        "return" => Some(TokenType.RETURN),
        "if" => Some(TokenType.IF),
        "else" => Some(TokenType.ELSE),
        "for" => Some(TokenType.FOR),
        "while" => Some(TokenType.WHILE),
        _ => None
    }
}

fn tokenize(source: string, file: string) -> Result<[]Token> {
    lexer = new_lexer(source, file)
    return lexer.run()
}